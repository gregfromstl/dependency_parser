{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing with the Catalan \"An Cora\" UD Treebank\n",
    "For this challenge I created two models (using pure PyTorch rather than any NLP-specific library per the guidelines). One model is the \"oracle\" to predict \"shift\", \"leftArc\", or \"rightArc\" in the dependency parsing process. The other model determines the dependency relation column of each token. The two models are fairly similar in architecture, both using bi-directional multilayer LSTMs but with a few differences based on how the data is formatted for each scenario.\n",
    "\n",
    "The features I used are specified in the `PARAMS` variable (this was originally meant to be customizable but is not currently working so). Both models take in the features as a representation of each token and are fed into the LSTM to determine outputs. For the oracle, the features are fed in as the pair of the top two tokens in the stack at time *t* in the labeling process.\n",
    "\n",
    "At the end I use the models to determine LAS and UAS scores using the oracle's operators to determine the dependency heads and the dependency relation model's outputs directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "I started this challenge before Dr. Scannell provided his code for parsing the data, so I am using a library that does essentially the same thing. In addition I'm using vanilla PyTorch for the oracle and the dependency labeler. The datasets I create are subclasses of the PyTorch `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizable Parameters\n",
    "Besides the features and files, there's a set of customizable parameters for each model. These are all standard parameters I have used in past challenges such as learning rate, dropout, embedding size, hidden size, etc. All parameters have been manually tuned to the optimal accuracy on their respective models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'features': ['form', 'lemma', 'upos'],\n",
    "    'train': \"ca_ancora-ud-train.conllu\",\n",
    "    'dev': \"ca_ancora-ud-dev.conllu\",\n",
    "    'test': \"ca_ancora-ud-test.conllu\",\n",
    "    'oracle': {\n",
    "        'epochs': 20,\n",
    "        'lr': 0.001,\n",
    "        'dropout': 0.3,\n",
    "        'embedding': 500,\n",
    "        'hidden': 200,\n",
    "        'n_layers': 4,\n",
    "        'bidirectional': True\n",
    "    },\n",
    "    'deprel': {\n",
    "        'epochs': 20,\n",
    "        'lr': 0.001,\n",
    "        'dropout': 0.3,\n",
    "        'embedding': 500,\n",
    "        'hidden': 200,\n",
    "        'n_layers': 4,\n",
    "        'bidirectional': True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Dataset to Feed the Oracle\n",
    "The `OperatorDataset` constructs the data the oracle will be trained on. The function of importance here is `get_samples`. It determines the sequence of \"shift\", \"leftArc\", and \"rightArc\" operators necessary to reconstruct the dependencies. Each of these labels is paired with the features of the top two tokens in the stack, which are the tokens used to make the decision. When taken as part of a full list of tokens in sequential order the oracle's LSTM sees the entire order of operators in which the dependencies are determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperatorDataset(Dataset):\n",
    "    def __init__(self, filename, features=None):\n",
    "        file = open(filename, \"r\")\n",
    "        self.raw = file.read()\n",
    "        self.operators = self.construct_id_dict(['shift', 'rightArc', 'leftArc'])\n",
    "        self.parsed = self.clean_data(parse(self.raw))\n",
    "        self.features = self.get_features() if features is None else features\n",
    "        self.data = [self.get_samples(sentence, as_tensor=True) for sentence in self.parsed]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parsed)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        features = torch.stack([token[0] for token in tokens])\n",
    "        labels = torch.stack([token[1] for token in tokens])\n",
    "        return features, labels\n",
    "    \n",
    "    def get_features(self):\n",
    "        assert hasattr(self, 'parsed') and self.parsed is not None\n",
    "        features = set(['<unk>', '<ROOT>', '<pad>'])\n",
    "        for sentence in self.parsed:\n",
    "            for token in sentence:\n",
    "                for feature in PARAMS['features']:\n",
    "                    features.add(token[feature])\n",
    "        return self.construct_id_dict(features)\n",
    "    \n",
    "    def label_operator(self, stack, remaining_tokens):\n",
    "        if len(stack) >= 2 and stack[1]['head'] == stack[0]['id']:\n",
    "            stack.pop(1)\n",
    "            return stack, remaining_tokens, 'leftArc'\n",
    "        elif len(stack) >= 2 and stack[1]['id'] == stack[0]['head'] and stack[0]['id'] not in [tok['head'] for tok in remaining_tokens]:\n",
    "            stack.pop(0)\n",
    "            return stack, remaining_tokens, 'rightArc'\n",
    "        else:\n",
    "            stack.insert(0, remaining_tokens.pop(0))\n",
    "            return stack, remaining_tokens, 'shift'\n",
    "\n",
    "    def get_samples(self, tokens: list, as_tensor=True):\n",
    "        token_list = tokens.copy()\n",
    "        samples = []\n",
    "        # start with 2 roots so there are always 2 tokens in the stack, this keeps dimensions consistent\n",
    "        root = {'form': '<ROOT>', 'lemma': '<ROOT>', 'upos': '<ROOT>', 'head': None, 'id': 0}\n",
    "        stack = [root, root]\n",
    "        while len(stack) > 2 or len(token_list) > 0:\n",
    "            stack_top = stack[:2]\n",
    "            stack, token_list, op = self.label_operator(stack, token_list)\n",
    "            samples.append((stack_top, op))\n",
    "        return samples if not as_tensor else self.samples_to_tensor(samples)\n",
    "    \n",
    "    def samples_to_tensor(self, samples: list):\n",
    "        new_samples = []\n",
    "        for sample in samples:\n",
    "            features, operator = sample\n",
    "            new_sample = torch.LongTensor([])\n",
    "            for token in features:\n",
    "                cleaned_features = [token[feature] if token[feature] in self.features['feature_to_id'] else '<unk>' for feature in PARAMS['features']]\n",
    "                token_features = torch.LongTensor([self.features['feature_to_id'][f] for f in cleaned_features])\n",
    "                new_sample = torch.cat((new_sample, token_features), 0)\n",
    "            new_samples.append((new_sample, torch.LongTensor([self.operators['feature_to_id'][operator]])))\n",
    "        return new_samples\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_id_dict(set_of_features: set):\n",
    "        features = {\n",
    "            'feature_to_id': {feature: i for i, feature in enumerate(set_of_features)},\n",
    "            'id_to_feature': {i: feature for i, feature in enumerate(set_of_features)}\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def clean_data(self, data: list) -> list:\n",
    "        clean_data = []\n",
    "        for sentence in data:\n",
    "            bad_sentence = False\n",
    "            # catch nonprojective trees\n",
    "            try:\n",
    "                self.get_samples(sentence, as_tensor=False)\n",
    "            except:\n",
    "                bad_sentence = True\n",
    "            if not bad_sentence:\n",
    "                clean_data.append(sentence)\n",
    "        return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the operator datasets. The training set determines its own features based on the training data. The validation and test sets use the training set's features. If a feature not seen in the training set appears in the validation or test data, that feature is replaces with the '<unk>' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_train_set = OperatorDataset(PARAMS['train'])\n",
    "oracle_val_set = OperatorDataset(PARAMS['dev'], oracle_train_set.features)\n",
    "oracle_test_set = OperatorDataset(PARAMS['test'], oracle_train_set.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader creation. I left the batch size at the default of 1 for the sake of simplicity. Each sample drawn from the dataset is still a full sequential list comprising a single sentence in the data so the LSTM can operate properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_train_loader = DataLoader(oracle_train_set)\n",
    "oracle_val_loader = DataLoader(oracle_val_set)\n",
    "oracle_test_loader = DataLoader(oracle_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Oracle Model\n",
    "The oracle is a bi-directional multilayer LSTM. Before feeding to the LSTM, the model generates embeddings of the provided features then uses a 2D convolutional layer to reduce the `# features * embedding_size` to a single value. This way the LSTM is looking at a sequence of single values that each represent the top two tokens in the stack at any given time. From my understanding this reduction using Embedding + 2D convolution is fairly unconventional but it seems to be working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_size=500, hidden_size=200, lstm_layers=4, is_bidirectional=True):\n",
    "        super(Oracle, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.conv = nn.Conv2d(1, 1, [len(PARAMS['features'])*2, self.embedding_size])\n",
    "        self.lstm = nn.LSTM(1, hidden_size, num_layers=self.lstm_layers, bidirectional=self.is_bidirectional)\n",
    "        self.linear = nn.Linear(hidden_size if not self.is_bidirectional else hidden_size*2, self.output_size)\n",
    "        self.dropout = nn.Dropout(PARAMS['oracle']['dropout'])\n",
    "        self.relu = F.relu\n",
    "    \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        emb = self.embedding(inputs).unsqueeze(1)\n",
    "        emb = self.dropout(emb)\n",
    "        emb = self.conv(self.relu(emb)).squeeze(-1)\n",
    "        \n",
    "        outputs, hidden = self.lstm(self.relu(emb), hidden)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.linear(self.relu(outputs).squeeze(1))\n",
    "        \n",
    "        return F.softmax(outputs, dim=-1), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model using specified parameters. The vocab size is the number of unique features and is used to generate the embeddings. The output size is the number of unique operators, which is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(oracle_train_set.features['feature_to_id'])\n",
    "ORACLE_OUTPUT_SIZE = len(oracle_train_set.operators['feature_to_id'])\n",
    "\n",
    "oracle = Oracle(\n",
    "    VOCAB_SIZE, \n",
    "    ORACLE_OUTPUT_SIZE, \n",
    "    PARAMS['oracle']['embedding'], \n",
    "    PARAMS['oracle']['hidden'],\n",
    "    PARAMS['oracle']['n_layers'],\n",
    "    PARAMS['oracle']['bidirectional']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "I'm using the most standard loss function and optimizer I could to simplify things. Basic Cross Entropy Loss with an Adam optimizer. I also tried using a learning rate scheduler but wasn't able to achieve the same or better results with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_criterion = nn.CrossEntropyLoss().to(device)\n",
    "oracle_optimizer = torch.optim.Adam(oracle.parameters(), lr=PARAMS['oracle']['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_oracle(model, loader, criterion, optimizer):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for sentence in loader:\n",
    "        oracle_optimizer.zero_grad()\n",
    "        inputs, labels = sentence\n",
    "        outputs, hidden = model(inputs.squeeze(0).to(device))\n",
    "        \n",
    "        loss = criterion(outputs, labels.squeeze().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += torch.sum(torch.argmax(outputs, dim=-1).cpu() == labels.squeeze()).item() / labels.squeeze().shape[0]\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return running_loss / len(loader), running_acc / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_oracle(model, loader, criterion):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for sentence in loader:\n",
    "        inputs, labels = sentence\n",
    "        outputs, hidden = model(inputs.squeeze(0).to(device))\n",
    "        \n",
    "        loss = criterion(outputs, labels.squeeze().to(device))\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += torch.sum(torch.argmax(outputs, dim=-1).cpu() == labels.squeeze()).item() / labels.squeeze().shape[0]\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return running_loss / len(loader), running_acc / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest accuracy I was able to achieve predicting operators with the oracle model is **98.4%** on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 16\n",
      "Train Loss:\t0.570\t|  Train Accuracy:\t98.15%\n",
      "Val. Loss:\t0.571\t|  Val. Accuracy:\t98.01%\n",
      "\n",
      "\n",
      "EPOCH 17\n",
      "Train Loss:\t0.569\t|  Train Accuracy:\t98.19%\n",
      "Val. Loss:\t0.570\t|  Val. Accuracy:\t98.16%\n",
      "\n",
      "\n",
      "EPOCH 18\n",
      "Train Loss:\t0.568\t|  Train Accuracy:\t98.26%\n",
      "Val. Loss:\t0.569\t|  Val. Accuracy:\t98.24%\n",
      "\n",
      "\n",
      "EPOCH 19\n",
      "Train Loss:\t0.568\t|  Train Accuracy:\t98.30%\n",
      "Val. Loss:\t0.567\t|  Val. Accuracy:\t98.40%\n",
      "\n",
      "\n",
      "EPOCH 20\n",
      "Train Loss:\t0.567\t|  Train Accuracy:\t98.38%\n",
      "Val. Loss:\t0.568\t|  Val. Accuracy:\t98.33%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(PARAMS['oracle']['epochs']):\n",
    "    train_loss, train_acc = train_oracle(oracle, oracle_train_loader, oracle_criterion, oracle_optimizer)\n",
    "    val_loss, val_acc = eval_oracle(oracle, oracle_val_loader, oracle_criterion)\n",
    "    \n",
    "    print(\"EPOCH {}\".format(epoch+1))\n",
    "    print(\"Train Loss:\\t{:.3f}\\t|  Train Accuracy:\\t{:.2f}%\".format(train_loss, train_acc*100))\n",
    "    print(\"Val. Loss:\\t{:.3f}\\t|  Val. Accuracy:\\t{:.2f}%\".format(val_loss, val_acc*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Test Results (Not UAS/LAS)\n",
    "This is the accuracy for the operators, not the dependency heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:\t0.567\t|  Test Accuracy:\t98.44%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_oracle(oracle, oracle_test_loader, oracle_criterion)\n",
    "print(\"Test Loss:\\t{:.3f}\\t|  Test Accuracy:\\t{:.2f}%\".format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Relation Dataset\n",
    "The dependency dataset is much simpler than the operator dataset in its behavior. It still does have the operator functions to clean the data of any non-projective trees, but its actual outputs are simply tensors of the feature IDs for each token's features, with the label of their dependency relation in sequential order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DepRelDataset(Dataset):\n",
    "    def __init__(self, filename, features=None, dependency_labels=None):\n",
    "        file = open(filename, \"r\")\n",
    "        self.raw = file.read()\n",
    "        self.operators = self.construct_id_dict(['shift', 'rightArc', 'leftArc'])\n",
    "        self.parsed = self.clean_data(parse(self.raw))\n",
    "        if features is None:\n",
    "            self.features = self.get_features()\n",
    "            self.dependency_labels = self.get_dependency_labels()\n",
    "        else:\n",
    "            self.features = features\n",
    "            self.dependency_labels = dependency_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parsed)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_list = self.parsed[idx]\n",
    "        inputs_dict = {feature: [] for feature in PARAMS['features']}\n",
    "        dependency_labels = []\n",
    "        for token in token_list:\n",
    "            for feature in PARAMS['features']:\n",
    "                if token[feature] not in self.features['feature_to_id']:\n",
    "                    feature_id = self.features['feature_to_id']['<unk>']\n",
    "                else:\n",
    "                    feature_id = self.features['feature_to_id'][token[feature]]\n",
    "                inputs_dict[feature].append(feature_id)\n",
    "            if token['deprel'] not in self.dependency_labels['feature_to_id']:\n",
    "                dep_id = self.dependency_labels['feature_to_id']['<unk>']\n",
    "            else:\n",
    "                dep_id = self.dependency_labels['feature_to_id'][token['deprel']]\n",
    "            dependency_labels.append(dep_id)\n",
    "        \n",
    "        inputs = [feature for feature in inputs_dict.values()]\n",
    "        return torch.LongTensor(inputs), torch.LongTensor(dependency_labels)\n",
    "    \n",
    "    def get_features(self):\n",
    "        assert hasattr(self, 'parsed') and self.parsed is not None\n",
    "        features = set(['<unk>','<pad>'])\n",
    "        for sentence in self.parsed:\n",
    "            for token in sentence:\n",
    "                for feature in PARAMS['features']:\n",
    "                    features.add(token[feature])\n",
    "        return self.construct_id_dict(features)\n",
    "    \n",
    "    def get_dependency_labels(self):\n",
    "        assert hasattr(self, 'parsed') and self.parsed is not None\n",
    "        dependency_labels = set(['<unk>','<pad>'])\n",
    "        for sentence in self.parsed:\n",
    "            for token in sentence:\n",
    "                dependency_labels.add(token['deprel'])\n",
    "        return self.construct_id_dict(dependency_labels)\n",
    "    \n",
    "    def label_operator(self, stack, remaining_tokens):\n",
    "        if len(stack) >= 2 and stack[1]['head'] == stack[0]['id']:\n",
    "            stack.pop(1)\n",
    "            return stack, remaining_tokens, 'leftArc'\n",
    "        elif len(stack) >= 2 and stack[1]['id'] == stack[0]['head'] and stack[0]['id'] not in [tok['head'] for tok in remaining_tokens]:\n",
    "            stack.pop(0)\n",
    "            return stack, remaining_tokens, 'rightArc'\n",
    "        else:\n",
    "            stack.insert(0, remaining_tokens.pop(0))\n",
    "            return stack, remaining_tokens, 'shift'\n",
    "\n",
    "    def get_operators(self, tokens: list):\n",
    "        token_list = tokens.copy()\n",
    "        operators = []\n",
    "        stack = [{'head': None, 'id': 0}]\n",
    "        while len(stack) > 1 or len(token_list) > 0:\n",
    "            stack, token_list, op = self.label_operator(stack, token_list)\n",
    "            operators.append(op)\n",
    "        return [self.operators['feature_to_id'][op] for op in operators]\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_id_dict(set_of_features: set):\n",
    "        features = {\n",
    "            'feature_to_id': {feature: i for i, feature in enumerate(set_of_features)},\n",
    "            'id_to_feature': {i: feature for i, feature in enumerate(set_of_features)}\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def clean_data(self, data: list) -> list:\n",
    "        clean_data = []\n",
    "        for sentence in data:\n",
    "            bad_sentence = False\n",
    "            # catch nonprojective trees\n",
    "            try:\n",
    "                self.get_operators(sentence)\n",
    "            except:\n",
    "                bad_sentence = True\n",
    "            if not bad_sentence:\n",
    "                clean_data.append(sentence)\n",
    "        return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, the validation and test sets use the training set's features and in this case dependency relation labels as well. The dependency relation labels are just in case there are any labels in the test data that haven't been seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deprel_train_set = DepRelDataset(PARAMS['train'])\n",
    "deprel_val_set = DepRelDataset(PARAMS['dev'], deprel_train_set.features, deprel_train_set.dependency_labels)\n",
    "deprel_test_set = DepRelDataset(PARAMS['test'], deprel_train_set.features, deprel_train_set.dependency_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deprel_train_loader = DataLoader(deprel_train_set)\n",
    "deprel_val_loader = DataLoader(deprel_val_set)\n",
    "deprel_test_loader = DataLoader(deprel_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Relation Model\n",
    "The `DependencyModel` is almost identical to the oracle. (I think it actually is identical, but I kept them separate for the sake of customizing each to their specific task.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DependencyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_size=500, hidden_size=200, lstm_layers=4, is_bidirectional=True):\n",
    "        super(DependencyModel, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.conv = nn.Conv2d(1, 1, [len(PARAMS['features']), self.embedding_size])\n",
    "        self.lstm = nn.LSTM(1, hidden_size, num_layers=self.lstm_layers, bidirectional=self.is_bidirectional)\n",
    "        self.linear = nn.Linear(hidden_size if not self.is_bidirectional else hidden_size*2, self.output_size)\n",
    "        self.dropout = nn.Dropout(PARAMS['deprel']['dropout'])\n",
    "        self.relu = F.relu\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        emb = self.embedding(inputs)\n",
    "        emb = self.dropout(emb)\n",
    "        emb = self.conv(self.relu(emb)).squeeze(-1)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(self.relu(emb))\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.linear(self.relu(outputs).squeeze(1))\n",
    "        return F.softmax(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the `DependencyModel` just like the oracle and using the same features. This time use the dependency relation labels as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(deprel_train_set.features['feature_to_id'])\n",
    "DEP_OUTPUT_SIZE = len(deprel_train_set.dependency_labels['feature_to_id'])\n",
    "\n",
    "dependency_model = DependencyModel(\n",
    "    VOCAB_SIZE, \n",
    "    DEP_OUTPUT_SIZE, \n",
    "    PARAMS['deprel']['embedding'], \n",
    "    PARAMS['deprel']['hidden'],\n",
    "    PARAMS['deprel']['n_layers'],\n",
    "    PARAMS['deprel']['bidirectional']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "Same as with the oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependency_criterion = nn.CrossEntropyLoss().to(device)\n",
    "dependency_optimizer = torch.optim.Adam(dependency_model.parameters(), lr=PARAMS['deprel']['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dependency Relation Model\n",
    "This function is slightly different than the oracle training based on how the dimensions worked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deprel(model, loader, criterion, optimizer):\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for sentence in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, labels = sentence\n",
    "        \n",
    "        outputs = model(inputs.permute(2,0,1).to(device))\n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze().to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += torch.sum(torch.argmax(outputs, dim=-1).cpu() == labels.squeeze()).item() / labels.squeeze().shape[0]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return running_loss / len(loader), running_acc / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Dependency Relation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_deprel(model, loader, criterion):\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for sentence in loader:\n",
    "        inputs, labels = sentence\n",
    "        \n",
    "        outputs = model(inputs.permute(2,0,1).to(device))\n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze().to(device))\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += torch.sum(torch.argmax(outputs, dim=-1).cpu() == labels.squeeze()).item() / labels.squeeze().shape[0]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return running_loss / len(loader), running_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "Train Loss:\t3.029\t|  Train Accuracy:\t57.59%\n",
      "Val. Loss:\t2.928\t|  Val. Accuracy:\t67.77%\n",
      "\n",
      "\n",
      "EPOCH 2\n",
      "Train Loss:\t2.906\t|  Train Accuracy:\t69.79%\n",
      "Val. Loss:\t2.888\t|  Val. Accuracy:\t71.60%\n",
      "\n",
      "\n",
      "EPOCH 3\n",
      "Train Loss:\t2.884\t|  Train Accuracy:\t72.00%\n",
      "Val. Loss:\t2.876\t|  Val. Accuracy:\t72.74%\n",
      "\n",
      "\n",
      "EPOCH 4\n",
      "Train Loss:\t2.868\t|  Train Accuracy:\t73.56%\n",
      "Val. Loss:\t2.856\t|  Val. Accuracy:\t74.78%\n",
      "\n",
      "\n",
      "EPOCH 5\n",
      "Train Loss:\t2.863\t|  Train Accuracy:\t74.14%\n",
      "Val. Loss:\t2.825\t|  Val. Accuracy:\t77.87%\n",
      "\n",
      "\n",
      "EPOCH 6\n",
      "Train Loss:\t2.828\t|  Train Accuracy:\t77.58%\n",
      "Val. Loss:\t2.814\t|  Val. Accuracy:\t78.94%\n",
      "\n",
      "\n",
      "EPOCH 7\n",
      "Train Loss:\t2.795\t|  Train Accuracy:\t80.87%\n",
      "Val. Loss:\t2.839\t|  Val. Accuracy:\t76.47%\n",
      "\n",
      "\n",
      "EPOCH 8\n",
      "Train Loss:\t2.786\t|  Train Accuracy:\t81.80%\n",
      "Val. Loss:\t2.793\t|  Val. Accuracy:\t81.08%\n",
      "\n",
      "\n",
      "EPOCH 9\n",
      "Train Loss:\t2.780\t|  Train Accuracy:\t82.40%\n",
      "Val. Loss:\t2.769\t|  Val. Accuracy:\t83.40%\n",
      "\n",
      "\n",
      "EPOCH 10\n",
      "Train Loss:\t2.776\t|  Train Accuracy:\t82.77%\n",
      "Val. Loss:\t2.760\t|  Val. Accuracy:\t84.40%\n",
      "\n",
      "\n",
      "EPOCH 11\n",
      "Train Loss:\t2.769\t|  Train Accuracy:\t83.46%\n",
      "Val. Loss:\t2.757\t|  Val. Accuracy:\t84.69%\n",
      "\n",
      "\n",
      "EPOCH 12\n",
      "Train Loss:\t2.766\t|  Train Accuracy:\t83.73%\n",
      "Val. Loss:\t2.755\t|  Val. Accuracy:\t84.89%\n",
      "\n",
      "\n",
      "EPOCH 13\n",
      "Train Loss:\t2.766\t|  Train Accuracy:\t83.76%\n",
      "Val. Loss:\t2.756\t|  Val. Accuracy:\t84.79%\n",
      "\n",
      "\n",
      "EPOCH 14\n",
      "Train Loss:\t2.771\t|  Train Accuracy:\t83.25%\n",
      "Val. Loss:\t2.763\t|  Val. Accuracy:\t84.11%\n",
      "\n",
      "\n",
      "EPOCH 15\n",
      "Train Loss:\t2.763\t|  Train Accuracy:\t84.06%\n",
      "Val. Loss:\t2.755\t|  Val. Accuracy:\t84.79%\n",
      "\n",
      "\n",
      "EPOCH 16\n",
      "Train Loss:\t2.763\t|  Train Accuracy:\t84.07%\n",
      "Val. Loss:\t2.754\t|  Val. Accuracy:\t84.95%\n",
      "\n",
      "\n",
      "EPOCH 17\n",
      "Train Loss:\t2.759\t|  Train Accuracy:\t84.43%\n",
      "Val. Loss:\t2.750\t|  Val. Accuracy:\t85.34%\n",
      "\n",
      "\n",
      "EPOCH 18\n",
      "Train Loss:\t2.756\t|  Train Accuracy:\t84.72%\n",
      "Val. Loss:\t2.746\t|  Val. Accuracy:\t85.71%\n",
      "\n",
      "\n",
      "EPOCH 19\n",
      "Train Loss:\t2.753\t|  Train Accuracy:\t85.09%\n",
      "Val. Loss:\t2.752\t|  Val. Accuracy:\t85.13%\n",
      "\n",
      "\n",
      "EPOCH 20\n",
      "Train Loss:\t2.753\t|  Train Accuracy:\t85.12%\n",
      "Val. Loss:\t2.745\t|  Val. Accuracy:\t85.79%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(PARAMS['deprel']['epochs']):\n",
    "    train_loss, train_acc = train_deprel(dependency_model, deprel_train_loader, dependency_criterion, dependency_optimizer)\n",
    "    val_loss, val_acc = eval_deprel(dependency_model, deprel_val_loader, dependency_criterion)\n",
    "    \n",
    "    print(\"EPOCH {}\".format(epoch+1))\n",
    "    print(\"Train Loss:\\t{:.3f}\\t|  Train Accuracy:\\t{:.2f}%\".format(train_loss, train_acc*100))\n",
    "    print(\"Val. Loss:\\t{:.3f}\\t|  Val. Accuracy:\\t{:.2f}%\".format(val_loss, val_acc*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Relation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:\t2.747\t|  Test Accuracy:\t85.60%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_deprel(dependency_model, deprel_test_loader, dependency_criterion)\n",
    "print(\"Test Loss:\\t{:.3f}\\t|  Test Accuracy:\\t{:.2f}%\".format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Dependency Heads Using Operators\n",
    "The `get_head_corrects` function performs the evaluation of the tokens using the operators and compares the predicted heads to the ground truth labels found in the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_corrects(tokens, operator_preds: torch.Tensor, operator_dict: dict):\n",
    "    remaining_tokens = tokens.copy()\n",
    "    stack = [{'form': '<ROOT>', 'lemma': '<ROOT>', 'upos': '<ROOT>', 'head': None, 'id': 0}]\n",
    "    corrects = []\n",
    "    idx = 0\n",
    "    while (len(stack) > 1 or len(remaining_tokens) > 0) and idx < operator_preds.shape[0]:\n",
    "        if len(remaining_tokens) == 0:\n",
    "            operator = torch.argmax(operator_preds[idx][1:]).item()+1\n",
    "        elif len(stack) < 2:\n",
    "            operator = 0\n",
    "        else:\n",
    "            operator = torch.argmax(operator_preds[idx]).item()\n",
    "        \n",
    "        if operator_dict[operator] == \"shift\":\n",
    "            stack.insert(0, remaining_tokens.pop(0))\n",
    "        elif operator_dict[operator] == \"leftArc\":\n",
    "            corrects.append(stack[0]['id'] == stack[1]['head'])\n",
    "            stack.pop(1)\n",
    "        elif operator_dict[operator] == \"rightArc\":\n",
    "            corrects.append(stack[1]['id'] == stack[0]['head'])\n",
    "            stack.pop(0)\n",
    "        idx += 1\n",
    "    return corrects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Function for LAS and UAS\n",
    "The `score` function uses the predicted heads and dependency relations to determine the \"Labeled Attachment Score\" and the \"Unlabeled Attachment Score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(oracle_set, deprel_set):\n",
    "    running_las = 0\n",
    "    running_uas = 0\n",
    "    \n",
    "    assert len(oracle_set) == len(deprel_set)\n",
    "\n",
    "    for idx, sentence in enumerate(oracle_set.parsed):\n",
    "        deprel_inputs, deprel_labels = deprel_set[idx]\n",
    "        deprel_preds = dependency_model(deprel_inputs.unsqueeze(0).permute(2,0,1).to(device))\n",
    "        deprel_corrects = (torch.argmax(deprel_preds, dim=-1).cpu() == deprel_labels.squeeze()).tolist()\n",
    "\n",
    "        oracle_inputs, _ = oracle_set[idx]\n",
    "        oracle_preds = oracle(oracle_inputs.to(device))[0]\n",
    "        oracle_corrects = get_head_corrects(sentence, oracle_preds.cpu(), oracle_set.operators['id_to_feature'])\n",
    "\n",
    "        running_uas += sum(oracle_corrects) / max(1, len(oracle_corrects))\n",
    "        assert len(oracle_corrects) == len(deprel_corrects)\n",
    "        running_las += sum([oracle_correct and deprel_corrects[i] for i, oracle_correct in enumerate(oracle_corrects)]) / max(1, len(oracle_corrects))\n",
    "    \n",
    "    return running_uas / len(oracle_set.parsed), running_las / len(oracle_set.parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Unlabeled Attachment Score:\t86.82\t|  Train Labeled Attachment Score:\t75.42\n",
      "Val. Unlabeled Attachment Score:\t83.06\t|  Val. Labeled Attachment Score:\t71.96\n",
      "Test Unlabeled Attachment Score:\t85.60\t|  Test Labeled Attachment Score:\t73.93\n"
     ]
    }
   ],
   "source": [
    "UAS_train, LAS_train = score(oracle_train_set, deprel_train_set)\n",
    "UAS_val, LAS_val = score(oracle_val_set, deprel_val_set)\n",
    "UAS_test, LAS_test = score(oracle_test_set, deprel_test_set)\n",
    "print(\"Train Unlabeled Attachment Score:\\t{:.2f}\\t|  Train Labeled Attachment Score:\\t{:.2f}\".format(UAS_train*100, LAS_train*100))\n",
    "print(\"Val. Unlabeled Attachment Score:\\t{:.2f}\\t|  Val. Labeled Attachment Score:\\t{:.2f}\".format(UAS_val*100, LAS_val*100))\n",
    "print(\"Test Unlabeled Attachment Score:\\t{:.2f}\\t|  Test Labeled Attachment Score:\\t{:.2f}\".format(UAS_test*100, LAS_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final test UAS was **85.60** compared to the 87.3 reported [here](https://core.ac.uk/download/pdf/78635826.pdf). The LAS was **73.83** compared to their reported 84.9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
